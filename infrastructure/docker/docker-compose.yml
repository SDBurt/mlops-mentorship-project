services:
  #####################################
  # STREAMING SERVICES (Kafka)
  #####################################

  kafka-broker:
    image: apache/kafka:4.0.0
    platform: linux/amd64
    container_name: kafka-broker
    ports:
      - "9092:9092"
    networks:
      - local-iceberg-lakehouse
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-broker:29092,CONTROLLER://kafka-broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-broker:29093'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 1
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  #####################################
  # LAKEHOUSE SERVICES
  #####################################

  polaris:
    image: apache/polaris:1.1.0-incubating
    platform: linux/amd64
    ports:
      - "8181:8181"
      - "8182:8182"
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL_STS: http://minio:9000
      POLARIS_BOOTSTRAP_CREDENTIALS: ${POLARIS_REALM},${POLARIS_USER},${POLARIS_PASSWORD}
      polaris.features.DROP_WITH_PURGE_ENABLED: "true"
      polaris.realm-context.realms: ${POLARIS_REALM}
    volumes:
      - polaris-data:/app/persistence
    healthcheck:
      test:
        - "CMD-SHELL"
        - >
          curl -f http://localhost:8182/q/health >/dev/null 2>&1 ||
          timeout 1 bash -c "</dev/tcp/localhost/8181" 2>/dev/null ||
          exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    networks:
      - local-iceberg-lakehouse

  polaris-init:
    image: alpine:3.21
    container_name: polaris-init
    depends_on:
      polaris:
        condition: service_healthy
    env_file:
      - .env
    environment:
      POLARIS_URL: http://polaris:8181
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: ${AWS_ACCESS_KEY_ID}
      MINIO_SECRET_KEY: ${AWS_SECRET_ACCESS_KEY}
      MINIO_REGION: ${AWS_REGION}
      CATALOG_NAME: polariscatalog
      CLIENT_ID: ${POLARIS_USER}
      CLIENT_SECRET: ${POLARIS_PASSWORD}
      SCOPE: PRINCIPAL_ROLE:ALL
    volumes:
      - ./polaris/init-polaris.sh:/init-polaris.sh:ro
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        apk add --no-cache bash curl jq >/dev/null 2>&1
        chmod +x /init-polaris.sh
        /init-polaris.sh
    networks:
      - local-iceberg-lakehouse
    restart: "no"

  trino:
    image: trinodb/trino:478
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      TRINO_JVM_OPTS: -Xmx2G
      # S3 credentials for MinIO access (used by iceberg.properties via ${ENV:...})
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # Polaris OAuth credentials (used by iceberg.properties via ${ENV:...})
      # Format: username:password (constructed from POLARIS_USER:POLARIS_PASSWORD)
      POLARIS_OAUTH_CREDENTIAL: ${POLARIS_USER}:${POLARIS_PASSWORD}
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      - polaris
      - minio
    networks:
      - local-iceberg-lakehouse

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    env_file:
      - .env
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_DOMAIN: minio
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
    ports:
      - "9000:9000"
      - "9001:9001"
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - minio-data:/data
    networks:
      local-iceberg-lakehouse:
        aliases:
          - warehouse.minio

  minio-client:
    image: minio/mc:RELEASE.2025-05-21T01-59-54Z
    depends_on:
      - minio
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      until (mc alias set minio http://minio:9000
      ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do
      echo '...waiting...' && sleep 1; done;
      mc mb --ignore-existing minio/warehouse;
      mc anonymous set public minio/warehouse;
      tail -f /dev/null
      "
    networks:
      - local-iceberg-lakehouse

  #####################################
  # ORCHESTRATION SERVICES (Dagster)
  #####################################

  postgres:
    image: postgres:14
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse

  dagster-user-code:
    build:
      context: ../../orchestration-dagster
      dockerfile: ./Dockerfile
    image: dagster_user_code_image
    container_name: dagster_user_code
    restart: always
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
      DAGSTER_CURRENT_IMAGE: dagster_user_code_image
      # PyIceberg / Polaris / MinIO for IO manager
      PYICEBERG_CATALOG__DEFAULT__URI: http://polaris:8181/api/catalog
      PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: polariscatalog
      PYICEBERG_CATALOG__DEFAULT__TYPE: rest
      PYICEBERG_CATALOG__DEFAULT__CREDENTIAL: ${POLARIS_USER}:${POLARIS_PASSWORD}
      PYICEBERG_CATALOG__DEFAULT__SCOPE: PRINCIPAL_ROLE:ALL
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      PYICEBERG_CATALOG__DEFAULT__S3__PATH_STYLE_ACCESS: "true"
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # Trino connection for data quality monitoring
      TRINO_HOST: trino
      # Payments DB connection for batch ingestion
      PAYMENTS_DB_HOST: payments-db
      PAYMENTS_DB_PORT: 5432
      PAYMENTS_DB_USER: ${PAYMENTS_DB_USER:-payments}
      PAYMENTS_DB_PASSWORD: ${PAYMENTS_DB_PASSWORD:-payments}
      PAYMENTS_DB_NAME: ${PAYMENTS_DB_NAME:-payments}
      # DBT project path for transformations
      DBT_PROJECT_DIR: /app/dbt
      # Feature export to MinIO (S3-compatible) for Feast
      FEATURE_S3_BUCKET: features
      FEATURE_S3_ENDPOINT: http://minio:9000
    volumes:
      - ../../orchestration-dbt/dbt:/app/dbt
    expose:
      - "3030"
    healthcheck:
      test: ["CMD", "dagster", "api", "grpc-health-check", "-p", "3030"]
      timeout: 2s
      interval: 3s
      retries: 20
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      polaris:
        condition: service_started
      minio:
        condition: service_started

  dagster-webserver:
    build:
      context: ./dagster
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagster-webserver
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - dagster-io:/tmp/io_manager_storage
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      dagster-user-code:
        condition: service_healthy

  dagster-daemon:
    build:
      context: ./dagster
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagster-daemon
      - run
    restart: on-failure
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - dagster-io:/tmp/io_manager_storage
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      dagster-user-code:
        condition: service_started

  #####################################
  # TRAEFIK REVERSE PROXY
  #####################################

  traefik:
    image: traefik:v3.0
    container_name: traefik
    ports:
      - "8000:8000"    # Webhook endpoint
      - "8090:8080"    # Dashboard (dev only)
    volumes:
      - ../../payment-pipeline/docker/traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ../../payment-pipeline/docker/traefik/dynamic:/etc/traefik/dynamic:ro
    networks:
      - local-iceberg-lakehouse
    depends_on:
      kafka-broker:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - gateway

  #####################################
  # PROVIDER-SPECIFIC GATEWAYS
  #####################################

  stripe-gateway:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.stripe
    container_name: stripe-gateway
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-whsec_test_secret_for_dev}
      LOG_LEVEL: INFO
      KAFKA_CONNECTION_RETRIES: 10
      KAFKA_RETRY_DELAY: 2.0
      KAFKA_RETRY_MAX_DELAY: 30.0
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    profiles:
      - gateway

  square-gateway:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.square
    container_name: square-gateway
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      SQUARE_WEBHOOK_SIGNATURE_KEY: ${SQUARE_WEBHOOK_SIGNATURE_KEY:-sq_signature_key_for_dev}
      SQUARE_NOTIFICATION_URL: http://traefik:8000/webhooks/square/
      LOG_LEVEL: INFO
      KAFKA_CONNECTION_RETRIES: 10
      KAFKA_RETRY_DELAY: 2.0
      KAFKA_RETRY_MAX_DELAY: 30.0
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    profiles:
      - gateway

  adyen-gateway:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.adyen
    container_name: adyen-gateway
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      ADYEN_HMAC_KEY: ${ADYEN_HMAC_KEY:-44782DEF547AAA06C910C43932B1EB0C71FC68D9D0C057550C48EC2ACF6BA056}
      LOG_LEVEL: INFO
      KAFKA_CONNECTION_RETRIES: 10
      KAFKA_RETRY_DELAY: 2.0
      KAFKA_RETRY_MAX_DELAY: 30.0
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    profiles:
      - gateway

  braintree-gateway:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.braintree
    container_name: braintree-gateway
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      # Empty credentials enable dev mode (skips signature verification)
      BRAINTREE_ENVIRONMENT: ${BRAINTREE_ENVIRONMENT:-sandbox}
      BRAINTREE_MERCHANT_ID: ${BRAINTREE_MERCHANT_ID:-}
      BRAINTREE_PUBLIC_KEY: ${BRAINTREE_PUBLIC_KEY:-}
      BRAINTREE_PRIVATE_KEY: ${BRAINTREE_PRIVATE_KEY:-}
      LOG_LEVEL: INFO
      KAFKA_CONNECTION_RETRIES: 10
      KAFKA_RETRY_DELAY: 2.0
      KAFKA_RETRY_MAX_DELAY: 30.0
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    profiles:
      - gateway

  #####################################
  # PROVIDER-SPECIFIC SIMULATORS
  #####################################

  stripe-simulator:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.stripe
    container_name: stripe-simulator
    command: ["python", "-m", "simulator.main", "generate", "--rate", "2", "--duration", "7200"]
    environment:
      GATEWAY_URL: http://traefik:8000/webhooks/stripe/
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-whsec_test_secret_for_dev}
    depends_on:
      stripe-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - simulator

  square-simulator:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.square
    container_name: square-simulator
    command: ["python", "-m", "simulator.main", "square", "generate", "--rate", "2", "--duration", "7200"]
    environment:
      SQUARE_GATEWAY_URL: http://traefik:8000/webhooks/square/
      SQUARE_WEBHOOK_SIGNATURE_KEY: ${SQUARE_WEBHOOK_SIGNATURE_KEY:-sq_signature_key_for_dev}
      SQUARE_NOTIFICATION_URL: http://traefik:8000/webhooks/square/
    depends_on:
      square-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - simulator

  adyen-simulator:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.adyen
    container_name: adyen-simulator
    command: ["python", "-m", "simulator.main", "adyen", "generate", "--rate", "2", "--duration", "7200"]
    environment:
      ADYEN_GATEWAY_URL: http://traefik:8000/webhooks/adyen/
      ADYEN_HMAC_KEY: ${ADYEN_HMAC_KEY:-44782DEF547AAA06C910C43932B1EB0C71FC68D9D0C057550C48EC2ACF6BA056}
    depends_on:
      adyen-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - simulator

  braintree-simulator:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/gateway/Dockerfile.braintree
    container_name: braintree-simulator
    command: ["python", "-m", "simulator.main", "braintree", "generate", "--rate", "2", "--duration", "7200"]
    environment:
      BRAINTREE_GATEWAY_URL: http://traefik:8000/webhooks/braintree/
      # Empty credentials enable dev mode (skips signature verification)
      BRAINTREE_ENVIRONMENT: ${BRAINTREE_ENVIRONMENT:-sandbox}
      BRAINTREE_MERCHANT_ID: ${BRAINTREE_MERCHANT_ID:-}
      BRAINTREE_PUBLIC_KEY: ${BRAINTREE_PUBLIC_KEY:-}
      BRAINTREE_PRIVATE_KEY: ${BRAINTREE_PRIVATE_KEY:-}
    depends_on:
      braintree-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - simulator

  #####################################
  # PROVIDER-SPECIFIC NORMALIZERS
  #####################################

  stripe-normalizer:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/normalizer/Dockerfile.stripe
    container_name: stripe-normalizer
    environment:
      NORMALIZER_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      NORMALIZER_OUTPUT_TOPIC: payments.normalized
      NORMALIZER_DLQ_TOPIC: payments.validation.dlq
      NORMALIZER_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      stripe-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - normalizer

  square-normalizer:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/normalizer/Dockerfile.square
    container_name: square-normalizer
    environment:
      NORMALIZER_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      NORMALIZER_OUTPUT_TOPIC: payments.normalized
      NORMALIZER_DLQ_TOPIC: payments.validation.dlq
      NORMALIZER_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      square-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - normalizer

  adyen-normalizer:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/normalizer/Dockerfile.adyen
    container_name: adyen-normalizer
    environment:
      NORMALIZER_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      NORMALIZER_OUTPUT_TOPIC: payments.normalized
      NORMALIZER_DLQ_TOPIC: payments.validation.dlq
      NORMALIZER_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      adyen-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - normalizer

  braintree-normalizer:
    build:
      context: ../../payment-pipeline
      dockerfile: docker/normalizer/Dockerfile.braintree
    container_name: braintree-normalizer
    environment:
      NORMALIZER_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      NORMALIZER_OUTPUT_TOPIC: payments.normalized
      NORMALIZER_DLQ_TOPIC: payments.validation.dlq
      NORMALIZER_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      braintree-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - normalizer

  #####################################
  # TEMPORAL SERVICES (Workflow Orchestration)
  #####################################

  payments-db:
    image: postgres:14
    container_name: payments-db
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: ${PAYMENTS_DB_USER:-payments}
      POSTGRES_PASSWORD: ${PAYMENTS_DB_PASSWORD:-payments}
      POSTGRES_DB: ${PAYMENTS_DB_NAME:-payments}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${PAYMENTS_DB_USER:-payments} -d ${PAYMENTS_DB_NAME:-payments}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - payments-db-data:/var/lib/postgresql/data
      - ./payments-db/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - local-iceberg-lakehouse
    profiles:
      - orchestrator

  temporal-db:
    image: postgres:14
    container_name: temporal-db
    environment:
      POSTGRES_USER: ${TEMPORAL_DB_USER:-temporal}
      POSTGRES_PASSWORD: ${TEMPORAL_DB_PASSWORD:-temporal}
      POSTGRES_DB: ${TEMPORAL_DB_NAME:-temporal}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${TEMPORAL_DB_USER:-temporal} -d ${TEMPORAL_DB_NAME:-temporal}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - temporal-db-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse
    profiles:
      - orchestrator

  temporal:
    image: temporalio/auto-setup:1.24
    container_name: temporal
    ports:
      - "7233:7233"
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-db
    depends_on:
      temporal-db:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - orchestrator

  temporal-ui:
    image: temporalio/ui:2.26.2
    container_name: temporal-ui
    ports:
      - "8088:8080"
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
    depends_on:
      - temporal
    networks:
      - local-iceberg-lakehouse
    profiles:
      - orchestrator

  #####################################
  # ORCHESTRATOR SERVICE (Temporal Workflows)
  #####################################

  inference-service:
    build:
      context: ../../payment-pipeline
      dockerfile: Dockerfile.inference
    container_name: inference-service
    ports:
      - "8002:8002"
    environment:
      INFERENCE_LOG_LEVEL: INFO
      INFERENCE_MODEL_VERSION: mock-v1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - local-iceberg-lakehouse
    profiles:
      - orchestrator

  orchestrator:
    build:
      context: ../../payment-pipeline
      dockerfile: Dockerfile.orchestrator
    container_name: payment-orchestrator
    environment:
      ORCHESTRATOR_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      ORCHESTRATOR_INPUT_TOPIC: payments.normalized
      ORCHESTRATOR_DLQ_TOPIC: payments.validation.dlq
      ORCHESTRATOR_KAFKA_CONSUMER_GROUP: orchestrator-group
      ORCHESTRATOR_TEMPORAL_HOST: temporal:7233
      ORCHESTRATOR_TEMPORAL_NAMESPACE: default
      ORCHESTRATOR_TEMPORAL_TASK_QUEUE: payment-processing
      ORCHESTRATOR_INFERENCE_SERVICE_URL: http://inference-service:8002
      # Postgres for payment event storage (replaces direct Iceberg writes)
      ORCHESTRATOR_POSTGRES_HOST: payments-db
      ORCHESTRATOR_POSTGRES_PORT: 5432
      ORCHESTRATOR_POSTGRES_USER: ${PAYMENTS_DB_USER:-payments}
      ORCHESTRATOR_POSTGRES_PASSWORD: ${PAYMENTS_DB_PASSWORD:-payments}
      ORCHESTRATOR_POSTGRES_DB: ${PAYMENTS_DB_NAME:-payments}
      # Iceberg config (kept for future batch loading)
      ORCHESTRATOR_ICEBERG_CATALOG_URI: http://polaris:8181/api/catalog
      ORCHESTRATOR_ICEBERG_WAREHOUSE: polariscatalog
      ORCHESTRATOR_ICEBERG_NAMESPACE: bronze
      ORCHESTRATOR_POLARIS_CLIENT_ID: ${POLARIS_USER}
      ORCHESTRATOR_POLARIS_CLIENT_SECRET: ${POLARIS_PASSWORD}
      ORCHESTRATOR_S3_ENDPOINT: http://minio:9000
      ORCHESTRATOR_S3_ACCESS_KEY: ${AWS_ACCESS_KEY_ID}
      ORCHESTRATOR_S3_SECRET_KEY: ${AWS_SECRET_ACCESS_KEY}
      ORCHESTRATOR_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      temporal:
        condition: service_started
      inference-service:
        condition: service_healthy
      payments-db:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - orchestrator

  #####################################
  # APACHE SUPERSET (BI Dashboard)
  #####################################

  superset-db:
    image: postgres:14
    container_name: superset-db
    environment:
      POSTGRES_USER: ${SUPERSET_DB_USER:-superset}
      POSTGRES_PASSWORD: ${SUPERSET_DB_PASSWORD:-superset}
      POSTGRES_DB: ${SUPERSET_DB_NAME:-superset}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${SUPERSET_DB_USER:-superset} -d ${SUPERSET_DB_NAME:-superset}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - superset-db-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse
    profiles:
      - superset

  superset-redis:
    image: redis:7
    container_name: superset-redis
    restart: unless-stopped
    volumes:
      - superset-redis-data:/data
    networks:
      - local-iceberg-lakehouse
    profiles:
      - superset

  superset-init:
    build:
      context: ./superset-config
      dockerfile: Dockerfile
    image: superset-lakehouse:latest
    container_name: superset-init
    env_file:
      - .env
    environment:
      DATABASE_HOST: superset-db
      DATABASE_PORT: 5432
      DATABASE_USER: ${SUPERSET_DB_USER:-superset}
      DATABASE_PASSWORD: ${SUPERSET_DB_PASSWORD:-superset}
      DATABASE_DB: ${SUPERSET_DB_NAME:-superset}
      REDIS_HOST: superset-redis
      REDIS_PORT: 6379
      SECRET_KEY: ${SUPERSET_SECRET_KEY:-CHANGE_ME_TO_A_COMPLEX_RANDOM_SECRET}
      SUPERSET_ADMIN_USERNAME: ${SUPERSET_ADMIN_USERNAME:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
    volumes:
      - ./superset-config/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ./superset-config/docker-init.sh:/app/docker-init.sh:ro
    entrypoint: ["/bin/bash", "/app/docker-init.sh"]
    depends_on:
      superset-db:
        condition: service_healthy
      superset-redis:
        condition: service_started
    networks:
      - local-iceberg-lakehouse
    profiles:
      - superset
    restart: "no"

  superset:
    build:
      context: ./superset-config
      dockerfile: Dockerfile
    image: superset-lakehouse:latest
    container_name: superset
    ports:
      - "8089:8088"  # Changed from 8088 to avoid conflict with Temporal UI
    env_file:
      - .env
    environment:
      DATABASE_HOST: superset-db
      DATABASE_PORT: 5432
      DATABASE_USER: ${SUPERSET_DB_USER:-superset}
      DATABASE_PASSWORD: ${SUPERSET_DB_PASSWORD:-superset}
      DATABASE_DB: ${SUPERSET_DB_NAME:-superset}
      REDIS_HOST: superset-redis
      REDIS_PORT: 6379
      SECRET_KEY: ${SUPERSET_SECRET_KEY:-CHANGE_ME_TO_A_COMPLEX_RANDOM_SECRET}
      # Trino connection for analytics
      TRINO_HOST: trino
      TRINO_PORT: 8080
    volumes:
      - ./superset-config/superset_config.py:/app/pythonpath/superset_config.py:ro
    command: ["/usr/bin/run-server.sh"]
    depends_on:
      superset-init:
        condition: service_completed_successfully
      trino:
        condition: service_started
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - superset

  superset-worker:
    build:
      context: ./superset-config
      dockerfile: Dockerfile
    image: superset-lakehouse:latest
    container_name: superset-worker
    env_file:
      - .env
    environment:
      DATABASE_HOST: superset-db
      DATABASE_PORT: 5432
      DATABASE_USER: ${SUPERSET_DB_USER:-superset}
      DATABASE_PASSWORD: ${SUPERSET_DB_PASSWORD:-superset}
      DATABASE_DB: ${SUPERSET_DB_NAME:-superset}
      REDIS_HOST: superset-redis
      REDIS_PORT: 6379
      SECRET_KEY: ${SUPERSET_SECRET_KEY:-CHANGE_ME_TO_A_COMPLEX_RANDOM_SECRET}
    volumes:
      - ./superset-config/superset_config.py:/app/pythonpath/superset_config.py:ro
    command: ["celery", "--app=superset.tasks.celery_app:app", "worker", "--pool=prefork", "-O", "fair", "-c", "4"]
    depends_on:
      superset-init:
        condition: service_completed_successfully
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - superset

  #####################################
  # MLOPS SERVICES (Feast + MLflow)
  #####################################

  # Initialize features bucket in MinIO for Feast
  feast-init:
    image: minio/mc:RELEASE.2025-05-21T01-59-54Z
    container_name: feast-init
    depends_on:
      minio:
        condition: service_started
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      until (mc alias set minio http://minio:9000
      ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do
      echo '...waiting for MinIO...' && sleep 1; done;
      mc mb --ignore-existing minio/features;
      echo 'Feast features bucket ready';
      "
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops
    restart: "no"

  # Redis for Feast online store
  feast-redis:
    image: redis:7
    container_name: feast-redis
    ports:
      - "6379:6379"
    volumes:
      - feast-redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops

  # PostgreSQL for Feast registry (separate from MLflow)
  feast-db:
    image: postgres:14
    container_name: feast-db
    environment:
      POSTGRES_USER: ${FEAST_DB_USER:-feast}
      POSTGRES_PASSWORD: ${FEAST_DB_PASSWORD:-feast}
      POSTGRES_DB: ${FEAST_DB_NAME:-feast_registry}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${FEAST_DB_USER:-feast} -d ${FEAST_DB_NAME:-feast_registry}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - feast-db-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops

  # Feast feature server
  feast-server:
    build:
      context: ../../mlops/feature-store
      dockerfile: Dockerfile
    container_name: feast-server
    ports:
      - "6566:6566"
    env_file:
      - .env
    environment:
      # Redis online store
      FEAST_REDIS_HOST: feast-redis
      FEAST_REDIS_PORT: 6379
      # PostgreSQL registry
      FEAST_REGISTRY_DB_HOST: feast-db
      FEAST_REGISTRY_DB_USER: ${FEAST_DB_USER:-feast}
      FEAST_REGISTRY_DB_PASSWORD: ${FEAST_DB_PASSWORD:-feast}
      FEAST_REGISTRY_DB_NAME: ${FEAST_DB_NAME:-feast_registry}
      # S3/MinIO for feature data
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_ENDPOINT_URL: http://minio:9000
      FEATURE_S3_BUCKET: features
    depends_on:
      feast-redis:
        condition: service_healthy
      feast-db:
        condition: service_healthy
      feast-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6566/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops

  # PostgreSQL for MLflow backend store
  mlflow-db:
    image: postgres:14
    container_name: mlflow-db
    environment:
      POSTGRES_USER: ${MLFLOW_DB_USER:-mlflow}
      POSTGRES_PASSWORD: ${MLFLOW_DB_PASSWORD:-mlflow}
      POSTGRES_DB: ${MLFLOW_DB_NAME:-mlflow}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${MLFLOW_DB_USER:-mlflow} -d ${MLFLOW_DB_NAME:-mlflow}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - mlflow-db-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops

  # MLflow tracking server
  mlflow-server:
    build:
      context: ./mlflow
      dockerfile: Dockerfile
    image: mlflow-server:latest
    container_name: mlflow-server
    ports:
      - "5001:5000"
    environment:
      # Backend store (PostgreSQL)
      MLFLOW_BACKEND_STORE_URI: postgresql://${MLFLOW_DB_USER:-mlflow}:${MLFLOW_DB_PASSWORD:-mlflow}@mlflow-db:5432/${MLFLOW_DB_NAME:-mlflow}
      # Artifact store (MinIO/S3)
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # Server config
      MLFLOW_TRACKING_URI: http://localhost:5000
    command:
      - mlflow
      - server
      - --host
      - "0.0.0.0"
      - --port
      - "5000"
      - --backend-store-uri
      - postgresql://${MLFLOW_DB_USER:-mlflow}:${MLFLOW_DB_PASSWORD:-mlflow}@mlflow-db:5432/${MLFLOW_DB_NAME:-mlflow}
      - --artifacts-destination
      - s3://warehouse/mlflow-artifacts
      - --serve-artifacts
    depends_on:
      mlflow-db:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops

  # MLflow artifact bucket initialization
  mlflow-init:
    image: minio/mc:RELEASE.2025-05-21T01-59-54Z
    container_name: mlflow-init
    depends_on:
      minio:
        condition: service_started
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      until (mc alias set minio http://minio:9000
      ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do
      echo '...waiting for MinIO...' && sleep 1; done;
      mc mb --ignore-existing minio/warehouse/mlflow-artifacts;
      echo 'MLflow artifact bucket ready';
      "
    networks:
      - local-iceberg-lakehouse
    profiles:
      - mlops
    restart: "no"

volumes:
  postgres-data:
  dagster-io:
  temporal-db-data:
  payments-db-data:
  superset-db-data:
  superset-redis-data:
  minio-data:
  polaris-data:
  feast-redis-data:
  feast-db-data:  # PostgreSQL for Feast registry
  mlflow-db-data:

networks:
  local-iceberg-lakehouse:
    name: local-iceberg-lakehouse
