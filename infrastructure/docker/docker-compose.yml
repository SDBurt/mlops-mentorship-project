services:
  #####################################
  # STREAMING SERVICES (Kafka + Flink)
  #####################################

  kafka-broker:
    image: apache/kafka:4.0.0
    platform: linux/amd64
    container_name: kafka-broker
    ports:
      - "9092:9092"
    networks:
      - local-iceberg-lakehouse
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-broker:29092,CONTROLLER://kafka-broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-broker:29093'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 1
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  flink-jobmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    ports:
      - "8081:8081"
    command: jobmanager
    networks:
      - local-iceberg-lakehouse
    depends_on:
      - polaris
      - minio

    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        # Configure checkpoint storage to use S3/MinIO
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: s3a://warehouse/flink-checkpoints
        # Configure S3 filesystem (Hadoop S3A) for MinIO
        s3.access-key: ${AWS_ACCESS_KEY_ID}
        s3.secret-key: ${AWS_SECRET_ACCESS_KEY}
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        # Hadoop S3A filesystem configuration
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: ${AWS_ACCESS_KEY_ID}
        fs.s3a.secret.key: ${AWS_SECRET_ACCESS_KEY}
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false

  flink-taskmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    command: taskmanager
    networks:
      - local-iceberg-lakehouse
    depends_on:
      - polaris
      - minio
    deploy:
      replicas: 1
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 8
        # Configure checkpoint storage to use S3/MinIO
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: s3a://warehouse/flink-checkpoints
        # Configure S3 filesystem (Hadoop S3A) for MinIO
        s3.access-key: ${AWS_ACCESS_KEY_ID}
        s3.secret-key: ${AWS_SECRET_ACCESS_KEY}
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        # Hadoop S3A filesystem configuration
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: ${AWS_ACCESS_KEY_ID}
        fs.s3a.secret.key: ${AWS_SECRET_ACCESS_KEY}
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false

  flink-sql-client:
    build:
      context: .
      dockerfile: flink/Dockerfile
    command: ["sql-client.sh"]
    networks:
      - local-iceberg-lakehouse
    depends_on:
      polaris:
        condition: service_started
      minio:
        condition: service_started
      flink-jobmanager:
        condition: service_started
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
      # Flink SQL job configuration (parameterized in SQL files)
      POLARIS_URI: ${POLARIS_URI:-http://polaris:8181/api/catalog}
      POLARIS_OAUTH_URI: ${POLARIS_OAUTH_URI:-http://polaris:8181/api/catalog/v1/oauth/tokens}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS:-kafka-broker:29092}
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        rest.address: flink-jobmanager
        # Configure checkpoint storage to use S3/MinIO
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: s3a://warehouse/flink-checkpoints
        # Configure S3 filesystem (Hadoop S3A) for MinIO
        s3.access-key: ${AWS_ACCESS_KEY_ID}
        s3.secret-key: ${AWS_SECRET_ACCESS_KEY}
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        # Hadoop S3A filesystem configuration
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: ${AWS_ACCESS_KEY_ID}
        fs.s3a.secret.key: ${AWS_SECRET_ACCESS_KEY}
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false
    stdin_open: true
    tty: true

  flink-job-submitter:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink-job-submitter
    command: ["/opt/flink/entrypoint-submit.sh"]
    networks:
      - local-iceberg-lakehouse
    depends_on:
      flink-jobmanager:
        condition: service_started
      kafka-broker:
        condition: service_healthy
      polaris:
        condition: service_started
    volumes:
      - ./flink/sql:/opt/flink/sql
      - ./flink/entrypoint-submit.sh:/opt/flink/entrypoint-submit.sh
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
      FLINK_JOBMANAGER: flink-jobmanager:8081
      KAFKA_BROKER: kafka-broker:29092
      POLARIS_HOST: polaris:8181
      POLARIS_USER: ${POLARIS_USER}
      POLARIS_PASSWORD: ${POLARIS_PASSWORD}
      # Flink properties for S3 access
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        rest.address: flink-jobmanager
        execution.checkpointing.storage: filesystem
        execution.checkpointing.dir: s3a://warehouse/flink-checkpoints
        s3.access-key: ${AWS_ACCESS_KEY_ID}
        s3.secret-key: ${AWS_SECRET_ACCESS_KEY}
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: ${AWS_ACCESS_KEY_ID}
        fs.s3a.secret.key: ${AWS_SECRET_ACCESS_KEY}
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false

  #####################################
  # LAKEHOUSE SERVICES
  #####################################

  polaris:
    image: apache/polaris:latest
    platform: linux/amd64
    ports:
      - "8181:8181"
      - "8182:8182"
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL_STS: http://minio:9000
      POLARIS_BOOTSTRAP_CREDENTIALS: ${POLARIS_REALM},${POLARIS_USER},${POLARIS_PASSWORD}
      polaris.features.DROP_WITH_PURGE_ENABLED: "true"
      polaris.realm-context.realms: ${POLARIS_REALM}
    healthcheck:
      test:
        - "CMD-SHELL"
        - >
          curl -f http://localhost:8181/api/catalog/v1/config >/dev/null 2>&1 ||
          curl -f http://localhost:8182/health >/dev/null 2>&1 ||
          timeout 1 bash -c "</dev/tcp/localhost/8182" 2>/dev/null ||
          exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    networks:
      - local-iceberg-lakehouse

  polaris-init:
    image: alpine:latest
    container_name: polaris-init
    depends_on:
      polaris:
        condition: service_healthy
    env_file:
      - .env
    environment:
      POLARIS_URL: http://polaris:8181
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: ${AWS_ACCESS_KEY_ID}
      MINIO_SECRET_KEY: ${AWS_SECRET_ACCESS_KEY}
      MINIO_REGION: ${AWS_REGION}
      CATALOG_NAME: polariscatalog
      CLIENT_ID: ${POLARIS_USER}
      CLIENT_SECRET: ${POLARIS_PASSWORD}
      SCOPE: PRINCIPAL_ROLE:ALL
    volumes:
      - ./polaris/init-polaris.sh:/init-polaris.sh:ro
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        apk add --no-cache bash curl jq >/dev/null 2>&1
        chmod +x /init-polaris.sh
        /init-polaris.sh
    networks:
      - local-iceberg-lakehouse
    restart: "no"

  trino:
    image: trinodb/trino:latest
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      TRINO_JVM_OPTS: -Xmx2G
      # S3 credentials for MinIO access (used by iceberg.properties via ${ENV:...})
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # Polaris OAuth credentials (used by iceberg.properties via ${ENV:...})
      # Format: username:password (constructed from POLARIS_USER:POLARIS_PASSWORD)
      POLARIS_OAUTH_CREDENTIAL: ${POLARIS_USER}:${POLARIS_PASSWORD}
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      - polaris
      - minio
    networks:
      - local-iceberg-lakehouse

  minio:
    image: minio/minio:latest
    env_file:
      - .env
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_DOMAIN: minio
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
    ports:
      - "9000:9000"
      - "9001:9001"
    command: ["server", "/data", "--console-address", ":9001"]
    networks:
      local-iceberg-lakehouse:
        aliases:
          - warehouse.minio

  minio-client:
    image: minio/mc:latest
    depends_on:
      - minio
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      until (mc alias set minio http://minio:9000
      ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do
      echo '...waiting...' && sleep 1; done;
      mc rm -r --force minio/warehouse;
      mc mb minio/warehouse;
      mc anonymous set public minio/warehouse;
      tail -f /dev/null
      "
    networks:
      - local-iceberg-lakehouse

  #####################################
  # ORCHESTRATION SERVICES (Dagster)
  #####################################

  postgres:
    image: postgres:14
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - local-iceberg-lakehouse

  dagster-user-code:
    build:
      context: ../../orchestration-dagster
      dockerfile: ./Dockerfile
    image: dagster_user_code_image
    container_name: dagster_user_code
    restart: always
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
      DAGSTER_CURRENT_IMAGE: dagster_user_code_image
      # PyIceberg / Polaris / MinIO for IO manager
      PYICEBERG_CATALOG__DEFAULT__URI: http://polaris:8181/api/catalog
      PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: polariscatalog
      PYICEBERG_CATALOG__DEFAULT__TYPE: rest
      PYICEBERG_CATALOG__DEFAULT__CREDENTIAL: ${POLARIS_USER}:${POLARIS_PASSWORD}
      PYICEBERG_CATALOG__DEFAULT__SCOPE: PRINCIPAL_ROLE:ALL
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      PYICEBERG_CATALOG__DEFAULT__S3__PATH_STYLE_ACCESS: "true"
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # Trino connection for data quality monitoring
      TRINO_HOST: trino
    expose:
      - "3030"
    healthcheck:
      test: ["CMD", "dagster", "api", "grpc-health-check", "-p", "3030"]
      timeout: 2s
      interval: 3s
      retries: 20
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      polaris:
        condition: service_started
      minio:
        condition: service_started

  dagster-webserver:
    build:
      context: ./dagster
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagster-webserver
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - dagster-io:/tmp/io_manager_storage
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      dagster-user-code:
        condition: service_healthy

  dagster-daemon:
    build:
      context: ./dagster
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagster-daemon
      - run
    restart: on-failure
    env_file:
      - .env
    environment:
      DAGSTER_POSTGRES_USER: ${DAGSTER_POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      DAGSTER_POSTGRES_DB: ${DAGSTER_POSTGRES_DB}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - dagster-io:/tmp/io_manager_storage
    networks:
      - local-iceberg-lakehouse
    depends_on:
      postgres:
        condition: service_healthy
      dagster-user-code:
        condition: service_started

  #####################################
  # JR GENERATOR SERVICES
  #####################################

  jr-charges:
    build:
      context: ./jr
      dockerfile: Dockerfile
    container_name: jr-charges
    networks:
      - local-iceberg-lakehouse
    volumes:
      - ./jr:/home/jr-user/templates
    depends_on:
      kafka-broker:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - generators

  jr-refunds:
    build:
      context: ./jr
      dockerfile: Dockerfile
    container_name: jr-refunds
    networks:
      - local-iceberg-lakehouse
    volumes:
      - ./jr:/home/jr-user/templates
    depends_on:
      kafka-broker:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - generators
    entrypoint: ["/usr/local/bin/generate-refunds.sh"]

  jr-disputes:
    build:
      context: ./jr
      dockerfile: Dockerfile
    container_name: jr-disputes
    networks:
      - local-iceberg-lakehouse
    volumes:
      - ./jr:/home/jr-user/templates
    depends_on:
      kafka-broker:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - generators
    entrypoint: ["/usr/local/bin/generate-disputes.sh"]

  jr-subscriptions:
    build:
      context: ./jr
      dockerfile: Dockerfile
    container_name: jr-subscriptions
    networks:
      - local-iceberg-lakehouse
    volumes:
      - ./jr:/home/jr-user/templates
    depends_on:
      kafka-broker:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - generators
    entrypoint: ["/usr/local/bin/generate-subscriptions.sh"]

  #####################################
  # PAYMENT GATEWAY SERVICES
  #####################################

  payment-gateway:
    build:
      context: ../../payment-pipeline
      dockerfile: Dockerfile
    container_name: payment-gateway
    ports:
      - "8000:8000"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-whsec_test_secret_for_dev}
      LOG_LEVEL: INFO
      KAFKA_CONNECTION_RETRIES: 10
      KAFKA_RETRY_DELAY: 2.0
      KAFKA_RETRY_MAX_DELAY: 30.0
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    profiles:
      - gateway

  webhook-simulator:
    build:
      context: ../../payment-pipeline
      dockerfile: Dockerfile
    container_name: webhook-simulator
    command: ["python", "-m", "simulator.main", "generate", "--rate", "2", "--duration", "300"]
    environment:
      GATEWAY_URL: http://payment-gateway:8000/webhooks/stripe/
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-whsec_test_secret_for_dev}
    depends_on:
      payment-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    profiles:
      - simulator

  #####################################
  # NORMALIZER SERVICE (Python)
  #####################################

  normalizer:
    build:
      context: ../../payment-pipeline
      dockerfile: Dockerfile.normalizer
    container_name: payment-normalizer
    environment:
      NORMALIZER_KAFKA_BOOTSTRAP_SERVERS: kafka-broker:29092
      NORMALIZER_KAFKA_CONSUMER_GROUP: normalizer-group
      NORMALIZER_INPUT_TOPICS: webhooks.stripe.payment_intent,webhooks.stripe.charge,webhooks.stripe.refund
      NORMALIZER_OUTPUT_TOPIC: payments.normalized
      NORMALIZER_DLQ_TOPIC: payments.validation.dlq
      NORMALIZER_LOG_LEVEL: INFO
    depends_on:
      kafka-broker:
        condition: service_healthy
      payment-gateway:
        condition: service_healthy
    networks:
      - local-iceberg-lakehouse
    restart: unless-stopped
    profiles:
      - normalizer

volumes:
  postgres-data:
  dagster-io:

networks:
  local-iceberg-lakehouse:
    name: local-iceberg-lakehouse
