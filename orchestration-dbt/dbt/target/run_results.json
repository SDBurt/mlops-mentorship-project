{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v6.json", "dbt_version": "1.10.13", "generated_at": "2025-11-03T05:08:27.879945Z", "invocation_id": "ebf4bfa3-2844-45de-bcd1-11120e985e71", "invocation_started_at": "2025-11-03T05:08:15.801587Z", "env": {}}, "results": [{"status": "error", "timing": [{"name": "compile", "started_at": "2025-11-03T05:08:21.422107Z", "completed_at": "2025-11-03T05:08:24.179430Z"}, {"name": "execute", "started_at": "2025-11-03T05:08:24.184600Z", "completed_at": "2025-11-03T05:08:24.267052Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 2.910059690475464, "adapter_response": {}, "message": "Database Error in model stg_reddit_subreddits (models/staging/reddit/stg_reddit_subreddits.sql)\n  TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 28:13: mismatched input 'from'. Expecting: '*', <expression>\", query_id=20251103_050709_00034_j8y7i)\n  compiled code at target/run/lakehouse_analytics/models/staging/reddit/stg_reddit_subreddits.sql", "failures": null, "unique_id": "model.lakehouse_analytics.stg_reddit_subreddits", "compiled": true, "compiled_code": "-- ============================================================================\n-- Staging Model: Reddit Subreddits\n-- ============================================================================\n-- Purpose: Union all Reddit subreddit metadata from all sources\n-- Source: Raw subreddit metadata tables from 5 subreddit datasets\n-- Materialization: View (source-conformed, atomic building block)\n-- ============================================================================\n\n\n\nWITH all_subreddits AS (\n    -- Union subreddit metadata from all sources using dbt_utils\n    \n    \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_subreddit\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_subreddit\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_economics\".\"reddit_economics_subreddit\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_economics\".\"reddit_economics_subreddit\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_finance\".\"reddit_finance_subreddit\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_finance\".\"reddit_finance_subreddit\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_subreddit\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_subreddit\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_investing\".\"reddit_investing_subreddit\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_investing\".\"reddit_investing_subreddit\"\n\n            \n        )\n\n        \n),\n\ncleaned_subreddits AS (\n    SELECT\n        -- ====================================================================\n        -- Identity Fields\n        -- ====================================================================\n        CAST(id AS VARCHAR) AS subreddit_id,\n        CAST(name AS VARCHAR) AS subreddit_name,  -- Full name (prefixed with t5_)\n        CAST(display_name AS VARCHAR) AS subreddit,  -- Display name (e.g., \"worldnews\")\n\n        -- ====================================================================\n        -- Descriptive Fields\n        -- ====================================================================\n        CAST(title AS VARCHAR) AS title,\n        CAST(public_description AS VARCHAR) AS public_description,\n        CAST(description AS VARCHAR) AS description,\n        CAST(header_title AS VARCHAR) AS header_title,\n\n        -- ====================================================================\n        -- Community Metrics\n        -- ====================================================================\n        CAST(subscribers AS BIGINT) AS subscribers,\n        CAST(accounts_active AS BIGINT) AS active_users,\n\n        -- ====================================================================\n        -- Temporal Fields\n        -- ====================================================================\n        FROM_UNIXTIME(CAST(created_utc AS BIGINT)) AS created_at,\n\n        -- ====================================================================\n        -- Community Settings\n        -- ====================================================================\n        CAST(over18 AS BOOLEAN) AS is_over_18,\n        CAST(subreddit_type AS VARCHAR) AS subreddit_type,  -- public, private, restricted, etc.\n        CAST(submission_type AS VARCHAR) AS submission_type,  -- any, link, self\n\n        -- ====================================================================\n        -- Moderation Settings\n        -- ====================================================================\n        CAST(spoilers_enabled AS BOOLEAN) AS spoilers_enabled,\n        CAST(allow_images AS BOOLEAN) AS allow_images,\n        CAST(allow_videos AS BOOLEAN) AS allow_videos,\n        CAST(allow_polls AS BOOLEAN) AS allow_polls,\n\n        -- ====================================================================\n        -- Community Flags\n        -- ====================================================================\n        CAST(quarantine AS BOOLEAN) AS quarantine,\n        CAST(user_is_banned AS BOOLEAN) AS user_is_banned,\n        CAST(user_is_muted AS BOOLEAN) AS user_is_muted,\n        CAST(user_is_moderator AS BOOLEAN) AS user_is_moderator\n\n    FROM all_subreddits\n\n    -- Data quality filters\n    WHERE id IS NOT NULL\n      AND display_name IS NOT NULL\n)\n\nSELECT * FROM cleaned_subreddits", "relation_name": "\"lakehouse\".\"raw_staging\".\"stg_reddit_subreddits\"", "batch_results": null}, {"status": "error", "timing": [{"name": "compile", "started_at": "2025-11-03T05:08:21.409374Z", "completed_at": "2025-11-03T05:08:24.167968Z"}, {"name": "execute", "started_at": "2025-11-03T05:08:24.168380Z", "completed_at": "2025-11-03T05:08:24.258777Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 2.912534475326538, "adapter_response": {}, "message": "Database Error in model stg_reddit_posts (models/staging/reddit/stg_reddit_posts.sql)\n  TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 28:13: mismatched input 'from'. Expecting: '*', <expression>\", query_id=20251103_050709_00033_j8y7i)\n  compiled code at target/run/lakehouse_analytics/models/staging/reddit/stg_reddit_posts.sql", "failures": null, "unique_id": "model.lakehouse_analytics.stg_reddit_posts", "compiled": true, "compiled_code": "-- ============================================================================\n-- Staging Model: Reddit Posts\n-- ============================================================================\n-- Purpose: Union all Reddit posts from all subreddits into a single staging view\n-- Source: Raw posts tables from 5 subreddit datasets\n-- Materialization: View (source-conformed, atomic building block)\n-- ============================================================================\n\n\n\nWITH all_posts AS (\n    -- Union posts from all subreddit sources using dbt_utils\n    \n    \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_posts\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_posts\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_economics\".\"reddit_economics_posts\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_economics\".\"reddit_economics_posts\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_finance\".\"reddit_finance_posts\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_finance\".\"reddit_finance_posts\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_posts\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_posts\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_investing\".\"reddit_investing_posts\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_investing\".\"reddit_investing_posts\"\n\n            \n        )\n\n        \n),\n\ncleaned_posts AS (\n    SELECT\n        -- ====================================================================\n        -- Identity Fields\n        -- ====================================================================\n        CAST(id AS VARCHAR) AS post_id,\n        CAST(name AS VARCHAR) AS post_name,\n        CAST(subreddit AS VARCHAR) AS subreddit,\n        FROM_UNIXTIME(CAST(created_utc AS BIGINT)) AS created_at,\n        CAST(permalink AS VARCHAR) AS permalink,\n\n        -- ====================================================================\n        -- Content Fields\n        -- ====================================================================\n        CAST(title AS VARCHAR) AS title,\n        CAST(selftext AS VARCHAR) AS selftext,\n        CAST(url AS VARCHAR) AS url,\n        CAST(domain AS VARCHAR) AS domain,\n\n        -- ====================================================================\n        -- Engagement Metrics\n        -- ====================================================================\n        CAST(score AS BIGINT) AS score,\n        CAST(upvote_ratio AS DOUBLE) AS upvote_ratio,\n        CAST(num_comments AS BIGINT) AS num_comments,\n        CAST(num_crossposts AS BIGINT) AS num_crossposts,\n\n        -- ====================================================================\n        -- Awards\n        -- ====================================================================\n        CAST(total_awards_received AS BIGINT) AS total_awards_received,\n        CAST(gilded AS BIGINT) AS gilded,\n\n        -- ====================================================================\n        -- User Interaction Signals (nullable - user-specific)\n        -- ====================================================================\n        CAST(clicked AS BOOLEAN) AS clicked,\n        CAST(visited AS BOOLEAN) AS visited,\n        CAST(saved AS BOOLEAN) AS saved,\n        CAST(hidden AS BOOLEAN) AS hidden,\n\n        -- ====================================================================\n        -- Author Features\n        -- ====================================================================\n        CAST(author AS VARCHAR) AS author,\n        CAST(author_fullname AS VARCHAR) AS author_fullname,\n        CAST(author_premium AS BOOLEAN) AS author_premium,\n        CAST(author_flair_text AS VARCHAR) AS author_flair_text,\n\n        -- ====================================================================\n        -- Content Type Classification\n        -- ====================================================================\n        CAST(is_self AS BOOLEAN) AS is_self,\n        CAST(is_video AS BOOLEAN) AS is_video,\n        CAST(is_gallery AS BOOLEAN) AS is_gallery,\n        CAST(post_hint AS VARCHAR) AS post_hint,\n        CAST(over_18 AS BOOLEAN) AS over_18,\n\n        -- ====================================================================\n        -- Moderation/Quality Flags\n        -- ====================================================================\n        CAST(stickied AS BOOLEAN) AS stickied,\n        CAST(locked AS BOOLEAN) AS locked,\n        CAST(archived AS BOOLEAN) AS archived,\n        -- Edited is either FALSE or a timestamp\n        CASE\n            WHEN edited IS NULL OR edited = false THEN FALSE\n            WHEN TYPEOF(edited) = 'boolean' THEN edited\n            ELSE TRUE\n        END AS edited\n\n    FROM all_posts\n\n    -- Data quality filters\n    WHERE id IS NOT NULL\n      AND subreddit IS NOT NULL\n      AND created_utc IS NOT NULL\n      AND score IS NOT NULL\n)\n\nSELECT * FROM cleaned_posts", "relation_name": "\"lakehouse\".\"raw_staging\".\"stg_reddit_posts\"", "batch_results": null}, {"status": "skipped", "timing": [], "thread_id": "Thread-4 (worker)", "execution_time": 0, "adapter_response": {}, "message": null, "failures": null, "unique_id": "model.lakehouse_analytics.dim_subreddit", "compiled": false, "compiled_code": null, "relation_name": "\"lakehouse\".\"raw_marts\".\"dim_subreddit\"", "batch_results": null}, {"status": "error", "timing": [{"name": "compile", "started_at": "2025-11-03T05:08:21.384970Z", "completed_at": "2025-11-03T05:08:24.276602Z"}, {"name": "execute", "started_at": "2025-11-03T05:08:24.282455Z", "completed_at": "2025-11-03T05:08:24.369477Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 3.0377392768859863, "adapter_response": {}, "message": "Database Error in model stg_reddit_comments (models/staging/reddit/stg_reddit_comments.sql)\n  TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 28:13: mismatched input 'from'. Expecting: '*', <expression>\", query_id=20251103_050709_00035_j8y7i)\n  compiled code at target/run/lakehouse_analytics/models/staging/reddit/stg_reddit_comments.sql", "failures": null, "unique_id": "model.lakehouse_analytics.stg_reddit_comments", "compiled": true, "compiled_code": "-- ============================================================================\n-- Staging Model: Reddit Comments\n-- ============================================================================\n-- Purpose: Union all Reddit comments from all subreddits into a single staging view\n-- Source: Raw comments tables from 5 subreddit datasets\n-- Materialization: View (source-conformed, atomic building block)\n-- ============================================================================\n\n\n\nWITH all_comments AS (\n    -- Union comments from all subreddit sources using dbt_utils\n    \n    \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_comments\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_worldnews\".\"reddit_worldnews_comments\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_economics\".\"reddit_economics_comments\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_economics\".\"reddit_economics_comments\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_finance\".\"reddit_finance_comments\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_finance\".\"reddit_finance_comments\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_comments\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_wallstreetbets\".\"reddit_wallstreetbets_comments\"\n\n            \n        )\n\n        union all\n        \n\n        (\n            select\n                cast('\"lakehouse\".\"raw_investing\".\"reddit_investing_comments\"' as varchar) as _dbt_source_relation,\n\n                \n\n            from \"lakehouse\".\"raw_investing\".\"reddit_investing_comments\"\n\n            \n        )\n\n        \n),\n\ncleaned_comments AS (\n    SELECT\n        -- ====================================================================\n        -- Identity Fields\n        -- ====================================================================\n        CAST(id AS VARCHAR) AS comment_id,\n        CAST(name AS VARCHAR) AS comment_name,\n        CAST(subreddit AS VARCHAR) AS subreddit,\n        FROM_UNIXTIME(CAST(created_utc AS BIGINT)) AS created_at,\n        CAST(permalink AS VARCHAR) AS permalink,\n\n        -- ====================================================================\n        -- Threading (Relationships)\n        -- Parse link_id and parent_id to extract IDs without type prefix (t3_, t1_)\n        -- link_id: Always starts with \"t3_\" (post type)\n        -- parent_id: Starts with \"t3_\" (post) or \"t1_\" (comment)\n        -- ====================================================================\n        CAST(link_id AS VARCHAR) AS link_id,\n        -- Extract post ID from link_id (remove \"t3_\" prefix)\n        CASE\n            WHEN SUBSTR(link_id, 1, 3) = 't3_' THEN SUBSTR(link_id, 4)\n            ELSE link_id\n        END AS post_id,\n\n        CAST(parent_id AS VARCHAR) AS parent_id,\n        -- Determine parent type (post or comment)\n        CASE\n            WHEN SUBSTR(parent_id, 1, 3) = 't3_' THEN 'post'\n            WHEN SUBSTR(parent_id, 1, 3) = 't1_' THEN 'comment'\n            ELSE 'unknown'\n        END AS parent_type,\n        -- Extract parent ID (remove type prefix)\n        CASE\n            WHEN SUBSTR(parent_id, 1, 3) IN ('t3_', 't1_') THEN SUBSTR(parent_id, 4)\n            ELSE parent_id\n        END AS parent_id_clean,\n\n        CAST(depth AS BIGINT) AS depth,\n\n        -- ====================================================================\n        -- Content Fields\n        -- ====================================================================\n        CAST(body AS VARCHAR) AS body,\n\n        -- ====================================================================\n        -- Engagement Metrics\n        -- ====================================================================\n        CAST(score AS BIGINT) AS score,\n        CAST(ups AS BIGINT) AS ups,\n        CAST(downs AS BIGINT) AS downs,\n        CAST(controversiality AS BIGINT) AS controversiality,\n\n        -- ====================================================================\n        -- Awards\n        -- ====================================================================\n        CAST(total_awards_received AS BIGINT) AS total_awards_received,\n        CAST(gilded AS BIGINT) AS gilded,\n\n        -- ====================================================================\n        -- Author Features\n        -- ====================================================================\n        CAST(author AS VARCHAR) AS author,\n        CAST(author_fullname AS VARCHAR) AS author_fullname,\n        CAST(author_premium AS BOOLEAN) AS author_premium,\n        CAST(author_flair_text AS VARCHAR) AS author_flair_text,\n        CAST(is_submitter AS BOOLEAN) AS is_submitter,  -- True if author is OP\n\n        -- ====================================================================\n        -- Moderation/Quality Flags\n        -- ====================================================================\n        CAST(stickied AS BOOLEAN) AS stickied,\n        CAST(distinguished AS VARCHAR) AS distinguished,  -- moderator, admin, special, NULL\n        CAST(score_hidden AS BOOLEAN) AS score_hidden,\n        -- Edited is either FALSE or a timestamp\n        CASE\n            WHEN edited IS NULL OR edited = false THEN FALSE\n            WHEN TYPEOF(edited) = 'boolean' THEN edited\n            ELSE TRUE\n        END AS edited\n\n    FROM all_comments\n\n    -- Data quality filters\n    WHERE id IS NOT NULL\n      AND subreddit IS NOT NULL\n      AND created_utc IS NOT NULL\n      AND link_id IS NOT NULL  -- Must have parent post\n)\n\nSELECT * FROM cleaned_comments", "relation_name": "\"lakehouse\".\"raw_staging\".\"stg_reddit_comments\"", "batch_results": null}, {"status": "skipped", "timing": [], "thread_id": "Thread-3 (worker)", "execution_time": 0, "adapter_response": {}, "message": null, "failures": null, "unique_id": "model.lakehouse_analytics.dim_date", "compiled": false, "compiled_code": null, "relation_name": "\"lakehouse\".\"raw_marts\".\"dim_date\"", "batch_results": null}, {"status": "skipped", "timing": [], "thread_id": "Thread-3 (worker)", "execution_time": 0, "adapter_response": {}, "message": null, "failures": null, "unique_id": "model.lakehouse_analytics.dim_author", "compiled": false, "compiled_code": null, "relation_name": "\"lakehouse\".\"raw_marts\".\"dim_author\"", "batch_results": null}, {"status": "skipped", "timing": [], "thread_id": "Thread-2 (worker)", "execution_time": 0, "adapter_response": {}, "message": null, "failures": null, "unique_id": "model.lakehouse_analytics.fct_posts", "compiled": false, "compiled_code": null, "relation_name": "\"lakehouse\".\"raw_marts\".\"fct_posts\"", "batch_results": null}, {"status": "skipped", "timing": [], "thread_id": "Thread-3 (worker)", "execution_time": 0, "adapter_response": {}, "message": null, "failures": null, "unique_id": "model.lakehouse_analytics.fct_comments", "compiled": false, "compiled_code": null, "relation_name": "\"lakehouse\".\"raw_marts\".\"fct_comments\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-11-03T05:08:21.369528Z", "completed_at": "2025-11-03T05:08:24.370672Z"}, {"name": "execute", "started_at": "2025-11-03T05:08:24.371330Z", "completed_at": "2025-11-03T05:08:27.874002Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 6.508878469467163, "adapter_response": {"_message": "CREATE TABLE (2_497 rows)", "rows_affected": 2497, "query_id": "20251103_050709_00036_j8y7i", "query": "/* {\"app\": \"dbt\", \"dbt_version\": \"1.10.13\", \"profile_name\": \"lakehouse\", \"target_name\": \"dev\", \"node_id\": \"model.lakehouse_analytics.dim_date_spine\"} */\n\n  \n    \n\n    create table \"lakehouse\".\"raw_marts\".\"dim_date_spine\"\n      \n      \n    as (\n      -- ============================================================================\n-- Dimension Model: Date Dimension (using date_spine)\n-- ============================================================================\n-- Purpose: Standard date dimension table using dbt_utils.date_spine\n-- Source: Generated date range (not dependent on data)\n-- Materialization: Table (persisted for performance)\n-- Surrogate Key: date_key (generated from date_day)\n--\n-- This version uses date_spine to generate a continuous date range,\n-- which is more robust than deriving dates from data.\n-- ============================================================================\n\n\n\nWITH date_spine AS (\n    -- Generate continuous date range using dbt_utils\n    -- Adjust start_date and end_date based on your needs\n    \n\n\n\n\n\nwith rawdata as (\n\n    \n\n    \n\n    with p as (\n        select 0 as generated_number union all select 1\n    ), unioned as (\n\n    select\n\n    \n    p0.generated_number * power(2, 0)\n     + \n    \n    p1.generated_number * power(2, 1)\n     + \n    \n    p2.generated_number * power(2, 2)\n     + \n    \n    p3.generated_number * power(2, 3)\n     + \n    \n    p4.generated_number * power(2, 4)\n     + \n    \n    p5.generated_number * power(2, 5)\n     + \n    \n    p6.generated_number * power(2, 6)\n     + \n    \n    p7.generated_number * power(2, 7)\n     + \n    \n    p8.generated_number * power(2, 8)\n     + \n    \n    p9.generated_number * power(2, 9)\n     + \n    \n    p10.generated_number * power(2, 10)\n     + \n    \n    p11.generated_number * power(2, 11)\n    \n    \n    + 1\n    as generated_number\n\n    from\n\n    \n    p as p0\n     cross join \n    \n    p as p1\n     cross join \n    \n    p as p2\n     cross join \n    \n    p as p3\n     cross join \n    \n    p as p4\n     cross join \n    \n    p as p5\n     cross join \n    \n    p as p6\n     cross join \n    \n    p as p7\n     cross join \n    \n    p as p8\n     cross join \n    \n    p as p9\n     cross join \n    \n    p as p10\n     cross join \n    \n    p as p11\n    \n    \n\n    )\n\n    select *\n    from unioned\n    where generated_number <= 2497\n    order by generated_number\n\n\n\n),\n\nall_periods as (\n\n    select (\n        date_add('day', row_number() over (order by 1) - 1, DATE '2020-01-01')\n    ) as date_day\n    from rawdata\n\n),\n\nfiltered as (\n\n    select *\n    from all_periods\n    where date_day <= current_date + interval '365' day\n\n)\n\nselect * from filtered\n\n\n),\n\ndate_dimension AS (\n    SELECT\n        -- ====================================================================\n        -- Surrogate Key\n        -- ====================================================================\n        lower(to_hex(md5(to_utf8(cast(coalesce(cast(date_day as varchar), '') as varchar))))) AS date_key,\n\n        -- ====================================================================\n        -- Natural Key\n        -- ====================================================================\n        date_day,\n\n        -- ====================================================================\n        -- Date Attributes\n        -- ====================================================================\n        EXTRACT(YEAR FROM date_day) AS year,\n        EXTRACT(QUARTER FROM date_day) AS quarter,\n        EXTRACT(MONTH FROM date_day) AS month,\n        EXTRACT(WEEK FROM date_day) AS week_of_year,\n        EXTRACT(DAY FROM date_day) AS day_of_month,\n        EXTRACT(DAY_OF_WEEK FROM date_day) AS day_of_week,  -- 1=Monday, 7=Sunday\n        EXTRACT(DAY_OF_YEAR FROM date_day) AS day_of_year,\n\n        -- ====================================================================\n        -- Formatted Labels\n        -- ====================================================================\n        CASE EXTRACT(DAY_OF_WEEK FROM date_day)\n            WHEN 1 THEN 'Monday'\n            WHEN 2 THEN 'Tuesday'\n            WHEN 3 THEN 'Wednesday'\n            WHEN 4 THEN 'Thursday'\n            WHEN 5 THEN 'Friday'\n            WHEN 6 THEN 'Saturday'\n            WHEN 7 THEN 'Sunday'\n        END AS day_name,\n\n        CASE EXTRACT(MONTH FROM date_day)\n            WHEN 1 THEN 'January'\n            WHEN 2 THEN 'February'\n            WHEN 3 THEN 'March'\n            WHEN 4 THEN 'April'\n            WHEN 5 THEN 'May'\n            WHEN 6 THEN 'June'\n            WHEN 7 THEN 'July'\n            WHEN 8 THEN 'August'\n            WHEN 9 THEN 'September'\n            WHEN 10 THEN 'October'\n            WHEN 11 THEN 'November'\n            WHEN 12 THEN 'December'\n        END AS month_name,\n\n        -- ====================================================================\n        -- Flags\n        -- ====================================================================\n        CASE\n            WHEN EXTRACT(DAY_OF_WEEK FROM date_day) IN (6, 7) THEN TRUE\n            ELSE FALSE\n        END AS is_weekend,\n\n        CASE\n            WHEN EXTRACT(DAY_OF_WEEK FROM date_day) BETWEEN 1 AND 5 THEN TRUE\n            ELSE FALSE\n        END AS is_weekday,\n\n        -- First day of month\n        CASE\n            WHEN EXTRACT(DAY FROM date_day) = 1 THEN TRUE\n            ELSE FALSE\n        END AS is_month_start,\n\n        -- Last day of month\n        CASE\n            WHEN date_day = DATE_ADD('month', 1, DATE_TRUNC('month', date_day)) - INTERVAL '1' DAY THEN TRUE\n            ELSE FALSE\n        END AS is_month_end,\n\n        -- ====================================================================\n        -- Period Labels\n        -- ====================================================================\n        FORMAT('%04d-Q%d', EXTRACT(YEAR FROM date_day), EXTRACT(QUARTER FROM date_day)) AS quarter_name,\n        FORMAT('%04d-%02d', EXTRACT(YEAR FROM date_day), EXTRACT(MONTH FROM date_day)) AS month_year,\n        FORMAT('%04d-W%02d', EXTRACT(YEAR FROM date_day), EXTRACT(WEEK FROM date_day)) AS week_year\n\n    FROM date_spine\n)\n\nSELECT * FROM date_dimension\n\n-- ============================================================================\n-- Benefits of using date_spine:\n-- ============================================================================\n-- 1. Continuous date range - no gaps in dates\n-- 2. Not dependent on data - works even if no data exists for certain dates\n-- 3. Can generate future dates for forecasting\n-- 4. Consistent grain (one row per day)\n-- 5. More efficient than deriving from fact tables\n-- ============================================================================\n    )"}, "message": "CREATE TABLE (2_497 rows)", "failures": null, "unique_id": "model.lakehouse_analytics.dim_date_spine", "compiled": true, "compiled_code": "-- ============================================================================\n-- Dimension Model: Date Dimension (using date_spine)\n-- ============================================================================\n-- Purpose: Standard date dimension table using dbt_utils.date_spine\n-- Source: Generated date range (not dependent on data)\n-- Materialization: Table (persisted for performance)\n-- Surrogate Key: date_key (generated from date_day)\n--\n-- This version uses date_spine to generate a continuous date range,\n-- which is more robust than deriving dates from data.\n-- ============================================================================\n\n\n\nWITH date_spine AS (\n    -- Generate continuous date range using dbt_utils\n    -- Adjust start_date and end_date based on your needs\n    \n\n\n\n\n\nwith rawdata as (\n\n    \n\n    \n\n    with p as (\n        select 0 as generated_number union all select 1\n    ), unioned as (\n\n    select\n\n    \n    p0.generated_number * power(2, 0)\n     + \n    \n    p1.generated_number * power(2, 1)\n     + \n    \n    p2.generated_number * power(2, 2)\n     + \n    \n    p3.generated_number * power(2, 3)\n     + \n    \n    p4.generated_number * power(2, 4)\n     + \n    \n    p5.generated_number * power(2, 5)\n     + \n    \n    p6.generated_number * power(2, 6)\n     + \n    \n    p7.generated_number * power(2, 7)\n     + \n    \n    p8.generated_number * power(2, 8)\n     + \n    \n    p9.generated_number * power(2, 9)\n     + \n    \n    p10.generated_number * power(2, 10)\n     + \n    \n    p11.generated_number * power(2, 11)\n    \n    \n    + 1\n    as generated_number\n\n    from\n\n    \n    p as p0\n     cross join \n    \n    p as p1\n     cross join \n    \n    p as p2\n     cross join \n    \n    p as p3\n     cross join \n    \n    p as p4\n     cross join \n    \n    p as p5\n     cross join \n    \n    p as p6\n     cross join \n    \n    p as p7\n     cross join \n    \n    p as p8\n     cross join \n    \n    p as p9\n     cross join \n    \n    p as p10\n     cross join \n    \n    p as p11\n    \n    \n\n    )\n\n    select *\n    from unioned\n    where generated_number <= 2497\n    order by generated_number\n\n\n\n),\n\nall_periods as (\n\n    select (\n        date_add('day', row_number() over (order by 1) - 1, DATE '2020-01-01')\n    ) as date_day\n    from rawdata\n\n),\n\nfiltered as (\n\n    select *\n    from all_periods\n    where date_day <= current_date + interval '365' day\n\n)\n\nselect * from filtered\n\n\n),\n\ndate_dimension AS (\n    SELECT\n        -- ====================================================================\n        -- Surrogate Key\n        -- ====================================================================\n        lower(to_hex(md5(to_utf8(cast(coalesce(cast(date_day as varchar), '') as varchar))))) AS date_key,\n\n        -- ====================================================================\n        -- Natural Key\n        -- ====================================================================\n        date_day,\n\n        -- ====================================================================\n        -- Date Attributes\n        -- ====================================================================\n        EXTRACT(YEAR FROM date_day) AS year,\n        EXTRACT(QUARTER FROM date_day) AS quarter,\n        EXTRACT(MONTH FROM date_day) AS month,\n        EXTRACT(WEEK FROM date_day) AS week_of_year,\n        EXTRACT(DAY FROM date_day) AS day_of_month,\n        EXTRACT(DAY_OF_WEEK FROM date_day) AS day_of_week,  -- 1=Monday, 7=Sunday\n        EXTRACT(DAY_OF_YEAR FROM date_day) AS day_of_year,\n\n        -- ====================================================================\n        -- Formatted Labels\n        -- ====================================================================\n        CASE EXTRACT(DAY_OF_WEEK FROM date_day)\n            WHEN 1 THEN 'Monday'\n            WHEN 2 THEN 'Tuesday'\n            WHEN 3 THEN 'Wednesday'\n            WHEN 4 THEN 'Thursday'\n            WHEN 5 THEN 'Friday'\n            WHEN 6 THEN 'Saturday'\n            WHEN 7 THEN 'Sunday'\n        END AS day_name,\n\n        CASE EXTRACT(MONTH FROM date_day)\n            WHEN 1 THEN 'January'\n            WHEN 2 THEN 'February'\n            WHEN 3 THEN 'March'\n            WHEN 4 THEN 'April'\n            WHEN 5 THEN 'May'\n            WHEN 6 THEN 'June'\n            WHEN 7 THEN 'July'\n            WHEN 8 THEN 'August'\n            WHEN 9 THEN 'September'\n            WHEN 10 THEN 'October'\n            WHEN 11 THEN 'November'\n            WHEN 12 THEN 'December'\n        END AS month_name,\n\n        -- ====================================================================\n        -- Flags\n        -- ====================================================================\n        CASE\n            WHEN EXTRACT(DAY_OF_WEEK FROM date_day) IN (6, 7) THEN TRUE\n            ELSE FALSE\n        END AS is_weekend,\n\n        CASE\n            WHEN EXTRACT(DAY_OF_WEEK FROM date_day) BETWEEN 1 AND 5 THEN TRUE\n            ELSE FALSE\n        END AS is_weekday,\n\n        -- First day of month\n        CASE\n            WHEN EXTRACT(DAY FROM date_day) = 1 THEN TRUE\n            ELSE FALSE\n        END AS is_month_start,\n\n        -- Last day of month\n        CASE\n            WHEN date_day = DATE_ADD('month', 1, DATE_TRUNC('month', date_day)) - INTERVAL '1' DAY THEN TRUE\n            ELSE FALSE\n        END AS is_month_end,\n\n        -- ====================================================================\n        -- Period Labels\n        -- ====================================================================\n        FORMAT('%04d-Q%d', EXTRACT(YEAR FROM date_day), EXTRACT(QUARTER FROM date_day)) AS quarter_name,\n        FORMAT('%04d-%02d', EXTRACT(YEAR FROM date_day), EXTRACT(MONTH FROM date_day)) AS month_year,\n        FORMAT('%04d-W%02d', EXTRACT(YEAR FROM date_day), EXTRACT(WEEK FROM date_day)) AS week_year\n\n    FROM date_spine\n)\n\nSELECT * FROM date_dimension\n\n-- ============================================================================\n-- Benefits of using date_spine:\n-- ============================================================================\n-- 1. Continuous date range - no gaps in dates\n-- 2. Not dependent on data - works even if no data exists for certain dates\n-- 3. Can generate future dates for forecasting\n-- 4. Consistent grain (one row per day)\n-- 5. More efficient than deriving from fact tables\n-- ============================================================================", "relation_name": "\"lakehouse\".\"raw_marts\".\"dim_date_spine\"", "batch_results": null}], "elapsed_time": 11.371148586273193, "args": {"warn_error_options": {"error": [], "warn": [], "silence": []}, "quiet": false, "favor_state": false, "partial_parse_file_diff": true, "static_parser": true, "log_path": "/home/sdburt/dev/learning/kubernetes/transformations/dbt/logs", "macro_debugging": false, "show_resource_report": false, "profiles_dir": "/home/sdburt/dev/learning/kubernetes/transformations/dbt", "require_nested_cumulative_type_params": false, "use_colors_file": true, "log_level": "info", "use_colors": true, "defer": false, "which": "run", "log_level_file": "debug", "upload_to_artifacts_ingest_api": false, "skip_nodes_if_on_run_start_fails": false, "state_modified_compare_vars": false, "printer_width": 80, "exclude": [], "cache_selected_only": false, "require_yaml_configuration_for_mf_time_spines": false, "strict_mode": false, "send_anonymous_usage_stats": true, "validate_macro_args": false, "vars": {}, "version_check": true, "state_modified_compare_more_unrendered_values": false, "populate_cache": true, "use_fast_test_edges": false, "require_all_warnings_handled_by_warn_error": false, "require_explicit_package_overrides_for_builtin_materializations": true, "indirect_selection": "eager", "introspect": true, "require_resource_names_without_spaces": true, "print": true, "partial_parse": true, "require_batched_execution_for_custom_microbatch_strategy": false, "source_freshness_run_project_hooks": true, "show_all_deprecations": false, "log_format": "default", "invocation_command": "dbt run", "log_file_max_bytes": 10485760, "log_format_file": "debug", "require_generic_test_arguments_property": true, "write_json": true, "select": [], "empty": false, "project_dir": "/home/sdburt/dev/learning/kubernetes/transformations/dbt"}}